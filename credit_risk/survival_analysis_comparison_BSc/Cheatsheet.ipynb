{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f611a4",
   "metadata": {},
   "source": [
    "# Cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard set of libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics \n",
    "np.random.seed(321) # just in case I forget later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/file/d/1Q9EgeAzyreI1tsCR7R9xBOXvJpQOEfWL/view?usp=sharing'\n",
    "url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "df = pd.read_csv(url)\n",
    "df_raw = df #create back up in case I mess up later\n",
    "df.columns = df.columns.str.replace(' ', '_') # change column names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ea362",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKUP\n",
    "df = pd.read_csv(\"NAME.csv\")\n",
    "df_raw = df #create back up in case I mess up later\n",
    "df.columns = df.columns.str.replace(' ', '_') # change column names "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88788602",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Columns except Target Variable\n",
    "features = np.setdiff1d(df.columns, ['TARGET']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Null values and remove them\n",
    "N_null = sum(df[features].isnull().sum())\n",
    "df = df.dropna()\n",
    "print(\"The raw_dataset contains {} null values\".format(N_null))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Variables from df\n",
    "to_remove = ['var1', 'var2', 'var3']\n",
    "df = df.drop(to_remove, axis = 1)\n",
    "\n",
    "#update features\n",
    "features = np.setdiff1d(features, to_remove).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select labels\n",
    "df_labels = df.columns[1:df.shape[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a8428",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517b2a4",
   "metadata": {},
   "source": [
    "MAKE SURE YOUR SCALED VARIABLES ARE NUMERIC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df[features])\n",
    "scaler.transform(df[features])\n",
    "df[features] = scaler.transform(df[features])\n",
    "df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b214544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Transfomration\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[features])\n",
    "scaler.transform(df[features])\n",
    "df[features] = scaler.transform(df[features])\n",
    "df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first need our function to handle outliers \n",
    "def outlier_truncation(x, factor=1.5):\n",
    "    x_new = x.copy()\n",
    "    IQR = x.quantile(0.75) - x.quantile(0.25) # Calculate IQR\n",
    "    # Define upper/lower bound\n",
    "    upper = x.quantile(0.75) + factor*IQR\n",
    "    lower = x.quantile(0.25) - factor*IQR\n",
    "    # Truncation\n",
    "    x_new[x < lower] = lower\n",
    "    x_new[x > upper] = upper\n",
    "    return x_new\n",
    "\n",
    "num_cols = [x for x in df.columns if df[x].dtype == 'float32' and x not in ['DUMMY OR BAD DISTRIBUTION (>75% = unique']]  \n",
    "df[num_cols] = df[num_cols].apply(outlier_truncation, axis=0, args=(3,))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c34de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoxCox Transformation\n",
    "from scipy import stats\n",
    "bc_fitted_feature, bc_fitted_lambda = stats.boxcox(df['var']+1) # Again, we are adding 1 because all features have 0s (not allowed in BC)\n",
    "bc_fitted_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9549b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yeo-Johson Transformation\n",
    "from scipy import stats\n",
    "yj_fitted_feature, yj_fitted_lambda = stats.yeojohnson(df['var'])\n",
    "yj_fitted_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1604e",
   "metadata": {},
   "source": [
    "### Dummy Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy encode features: 'Intl Plan', 'VMail Plan', and 'Area Code'\n",
    "to_dummy = ['Intl_Plan', 'VMail_Plan', 'Area_Code']\n",
    "\n",
    "df = pd.get_dummies(df, columns = to_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4bc30",
   "metadata": {},
   "source": [
    "### WOE Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scorecardpy as sc\n",
    "\n",
    "bins = sc.woebin(train_df, y=\"BAD\", x=['JOB', 'REASON'])\n",
    "sc.woebin_plot(bins)\n",
    "\n",
    "X_train_woe = sc.woebin_ply(X_train_scaled, bins)\n",
    "X_test_woe = sc.woebin_ply(X_test_scaled, bins)\n",
    "\n",
    "X_train_woe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafbba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scorecardpy as sc\n",
    "\n",
    "bins = sc.woebin(train_df, y=\"BAD\", x=['JOB', 'REASON'])\n",
    "sc.woebin_plot(bins)\n",
    "\n",
    "X_train_woe = sc.woebin_ply(X_train_scaled, bins)\n",
    "X_test_woe = sc.woebin_ply(X_test_scaled, bins)\n",
    "\n",
    "X_train = sc.woebin_ply(X_train, bins)\n",
    "X_test = sc.woebin_ply(X_test, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3419e",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe23ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6671c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "X_resampled, y_resampled = BorderlineSMOTE().fit_resample(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f4d81",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ef99f",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e96f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397df00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('TARGET')['VAR1', 'VAR2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc10e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute imbalance ratio\n",
    "freq = pd.crosstab(index=df['TARGET'], columns='count')\n",
    "print(freq)\n",
    "\n",
    "ir = freq['count'][0]/freq['count'][1]\n",
    "print('Imbalance ratio: ' ,ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96babf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEAN, leave out normalize for count\n",
    "reason = pd.crosstab(df.VAR1, df.VAR2, normalize='index')\n",
    "reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f9ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is a variable normally distributed?\n",
    "from scipy import stats\n",
    "\n",
    "stats.normaltest(df['VAR'])\n",
    "#If you would like a more scienfitic test for normality, you can use `normaltest` from `scipy`. This function's documentation is \n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html. It outputs 2 numbers, the first is the sum of \n",
    "# squares of the z-scores returned by a skewtest and kurtosistest. The second is a 2-sided chi2 probability hypothesis test. Thus, \n",
    "# if the p-value is below 0.05, it is likely you do not have normally distributed data.\n",
    "\n",
    "#OR\n",
    "\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "qqplot(feature, line='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c04daf8",
   "metadata": {},
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967d96b",
   "metadata": {},
   "source": [
    "CAREFUL: Some categorical variables CAN be numerical (telephone numbers for example). Make sure to exclude them with setdiff e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a421ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Numerical Variables only\n",
    "num_vars = df.select_dtypes(include=[np.float64, np.float32, np.int32, np.int64]).columns\n",
    "df[num_vars] = df[num_vars].astype(np.float32)\n",
    "\n",
    "#num_vars = np.setdiff1d(num_vars, ['CATEGORICALVARIABLE'])\n",
    "\n",
    "#\n",
    "df['JOB'] = df['JOB'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical Data and transform to category if not already category\n",
    "categories = df.select_dtypes(include=[np.object]).columns\n",
    "df[categories] = df[categories].astype(np.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de13c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Data based on Condition\n",
    "df.loc[df.BAD == 1, ['LOAN', 'JOB', 'YOJ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91525c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering Data:\n",
    "df.loc[df['BAD'] == 1]\n",
    "#syntax:\n",
    "# df.loc[condition]\n",
    "\n",
    "#Alternatively\n",
    "df.query('VAR > 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c496300",
   "metadata": {},
   "source": [
    "### Replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4149eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NA with anything, here mode e.g.\n",
    "df.VAR[df.VAR.isnull()] = df.VAR.mode()[0]  # the index [0] ensures that we only extract the value from the result of calling mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdbc4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace multiple numerical columns with median\n",
    "for col in df.select_dtypes(include='float32').columns:  # loop over all numeric columns\n",
    "    if df[col].isna().sum() > 0:                         # check if there are any missing values in the current feature\n",
    "        m = df[col].median(skipna=True)                  # compute the median of that feature\n",
    "        df[col].fillna(m, inplace=True)                  # replace missing values with the median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20270cf4",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432be98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 24,8\n",
    "# width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fa8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.crosstab(index=df['TARGET'], columns='count')\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.crosstab(index=df['TARGET'], columns='count')\n",
    "\n",
    "#test1 = freq.index.values #if categorical\n",
    "test1 =['0', '1']\n",
    "test2 = freq['count']\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "plt.title('Event Distribution')\n",
    "ax.bar(test1, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4984b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.crosstab(index=df['TARGET'], columns='count')\n",
    "\n",
    "#test1 = freq.index.values #if categorical\n",
    "test1 =['0', '1']\n",
    "test2 = freq['count']\n",
    "\n",
    "sns.barplot(x = test1, y = test2, ci= None)\n",
    "\n",
    "#https://python-graph-gallery.com/grouped-barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histographs for all numerical variables CHECK IF ALL VARIABLES ARE 'float64'\n",
    "df.select_dtypes(include='float64').hist(bins=30, figsize=(15, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3e856",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "corr= df.corr()\n",
    "f,ax = plt.subplots(figsize=(18, 15))\n",
    "sns.heatmap(corr ,annot=True,linewidth=.5,fmt='1f');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a03078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap with Threshold\n",
    "\n",
    "corr_threshold = 0.3\n",
    "f,ax = plt.subplots(figsize=(18, 15))\n",
    "sns.heatmap(corr[(corr >= corr_threshold) | (corr <= -corr_threshold)],\n",
    "            annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d75661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative Correlation Plot View\n",
    "heatmap = sns.heatmap(df.corr()[['Target']].sort_values(by='Target', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Features Correlating with Target', fontdict={'fontsize':18}, pad=16);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe12d67",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add a constant column to X. Think of this as the Python way to include an intercept in your model \n",
    "X = add_constant(X, prepend=True, has_constant='raise')\n",
    "\n",
    "#note we have to disable adding a constant in the models then (this is mostly default for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dfad82",
   "metadata": {},
   "source": [
    "List of Classifiers known:\n",
    "\n",
    "### Regression:\n",
    "sklearn.linear_model.LinearRegression\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "sklearn.linear_model.Lasso\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "sklearn.linear_model.LassoCV\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html\n",
    "\n",
    "sklearn.linear_model.Ridge\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge\n",
    "\n",
    "sklearn.linear_model.RidgeCV\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html\n",
    "\n",
    "sklearn.linear_model.ElasticNet\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "\n",
    "sklearn.linear_model.ElasticNetCV\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html\n",
    "\n",
    "sklearn.tree.DecisionTreeRegressor\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "\n",
    "sklearn.ensemble.RandomForestRegressor\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "sklearn.ensemble.GradientBoostingRegressor\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "\n",
    "### Classification:\n",
    "sklearn.linear_model.LogisticRegression\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "sklearn.linear_model.LogisticRegressionCV\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV\n",
    "\n",
    "sklearn.linear_model.RidgeClassifier\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html\n",
    "\n",
    "sklearn.tree.DecisionTreeClassifier\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "sklearn.ensemble.RandomForestClassifier\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "sklearn.ensemble.GradientBoostingClassifier\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc6010",
   "metadata": {},
   "source": [
    "### Basic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb8c27",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    \"learning_rate\": [1e-1, 1e-2], #obviously this is not the perfect approach, in real life setting I would suggest using small jumps and covering a wide range of learning rates as to avoid overshooting\n",
    "    \"max_depth\":[3,8]\n",
    "    }\n",
    "\n",
    "clf_gb = GridSearchCV(GradientBoostingClassifier(), parameters, scoring = 'f1', cv=3, n_jobs=-1)\n",
    "clf_gb.fit(X_train, y_train)\n",
    "y_gb = clf_gb.predict(X_test)\n",
    "y_gb_proba = clf_gb.predict_proba(X_test)[:,1]\n",
    "print(clf_gb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d91991",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed1a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    \"learning_rate\": [1e-1, 1e-2], #obviously this is not the perfect approach, in real life setting I would suggest using small jumps and covering a wide range of learning rates as to avoid overshooting\n",
    "    \"max_depth\":[3,8]\n",
    "    }\n",
    "\n",
    "clf_gb = GridSearchCV(GradientBoostingClassifier(), parameters, scoring = 'f1', cv=3, n_jobs=-1)\n",
    "clf_gb.fit(X_train, y_train)\n",
    "y_gb = clf_gb.predict(X_test)\n",
    "y_gb_proba = clf_gb.predict_proba(X_test)[:,1]\n",
    "print(clf_gb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb6cf9",
   "metadata": {},
   "source": [
    "### Advanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ccf7b",
   "metadata": {},
   "source": [
    "#### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    \"l1_ratio\": [0, 1],\n",
    "    \"C\":[0.2, 1]\n",
    "    }\n",
    "\n",
    "clf_lrreg = GridSearchCV(LogisticRegression(penalty = 'elasticnet', solver = 'saga', max_iter = 10000), parameters, scoring = 'f1', cv=3, n_jobs=-1)\n",
    "clf_lrreg.fit(X_train, y_train)\n",
    "y_lrreg = clf_lrreg.predict(X_test)\n",
    "y_lrreg_proba = clf_lrreg.predict_proba(X_test)[:,1]\n",
    "print(clf_lrreg.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50778eb",
   "metadata": {},
   "source": [
    "#### GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c2d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    \"learning_rate\": [1e-1, 1e-2], #obviously this is not the perfect approach, in real life setting I would suggest using small jumps and covering a wide range of learning rates as to avoid overshooting\n",
    "    \"max_depth\":[3,8]\n",
    "    }\n",
    "\n",
    "clf_gb = GridSearchCV(GradientBoostingClassifier(), parameters, scoring = 'f1', cv=3, n_jobs=-1)\n",
    "clf_gb.fit(X_train, y_train)\n",
    "y_gb = clf_gb.predict(X_test)\n",
    "y_gb_proba = clf_gb.predict_proba(X_test)[:,1]\n",
    "print(clf_gb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47e841",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81071cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Setting up the grid of meta-parameters\n",
    "xgb_param_grid = {\n",
    "    'colsample_bytree': np.linspace(0.5, 0.9, 5),  # random subspace\n",
    "    'n_estimators': [100, 200],  # ensemble size or number of gradient steps\n",
    "    'max_depth': [5, 10],   # max depth of decision trees\n",
    "    'learning_rate': [0.1, 0.01],  # learning rate\n",
    "    'early_stopping_rounds': [10]}  # early stopping if no improvement after that many iterations\n",
    "\n",
    "gs_xgb = GridSearchCV(estimator=xgb.XGBClassifier(), param_grid=xgb_param_grid, scoring='roc_auc', cv=5, verbose=0)\n",
    "gs_xgb.fit(X_train, y_train.values.ravel())\n",
    "gs_xgb_proba = gs_xgb.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b714e7d",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  # import library\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "print('Tuning random forest classifier')\n",
    "rf = RandomForestClassifier(random_state=888, max_samples = 0.5)  # This way, bootstrap sample size will be 50% of the training set\n",
    "\n",
    "# Define meta-parameter grid of candidate settings\n",
    "# The following settings are just for illustration\n",
    "param_grid = {'n_estimators': [100, 200, 500],\n",
    "              'max_features': [1, 2, 4]\n",
    "              }\n",
    "\n",
    "# Set up the grid object specifying the tuning options\n",
    "gs_rf = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc', verbose=1)\n",
    "gs_rf.fit(X_train, y_train.values.ravel())\n",
    "gs_rf_proba = gs_rf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9078239",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d7b52",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b475a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_lr.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "#AUC\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, clf_lr.predict_proba(X_test)[:,1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV Scoring\n",
    "\n",
    "from sklearn.model_selection import cross_validate \n",
    "\n",
    "# Add list of scoring parameters directly to the function (Acc)\n",
    "score = cross_validate(logit, X, y, scoring=p_measures, cv=10)\n",
    "score\n",
    "\n",
    "pd.DataFrame(score).mean()\n",
    "\n",
    "#OR \n",
    "lasso_scores = cross_val_score(lasso_sk, X, y.ravel(), cv=folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e8438",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc915c2",
   "metadata": {},
   "source": [
    "#### PrecisionRecallDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a18fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create the chart\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "display = PrecisionRecallDisplay.from_predictions(y_test, clf_dt.predict_proba(X_test)[:,1], name=\"Decision Tree\")\n",
    "_ = display.ax_.set_title(\"2-class Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3035bad",
   "metadata": {},
   "source": [
    "#### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8109fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.RocCurveDisplay.from_predictions(y_test, y_lr_proba, name = 'Logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_lr_proba)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                  estimator_name='ESTIMATOR NAME')\n",
    "display.plot()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c3170",
   "metadata": {},
   "source": [
    "### Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8223b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = model.predict(X_test)[:,1]\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.scatter(residuals,y_pred)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = residuals.mean()\n",
    "std = residuals.std()\n",
    "\n",
    "residuals = (residuals - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0cdbf1",
   "metadata": {},
   "source": [
    "### Manual Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model and get probaility predictions\n",
    "clf = LogisticRegression(penalty='none', fit_intercept=True).fit(X_train, y_train.ravel())\n",
    "pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Use the array above to manually determine the cut-off and convert to class predictions\n",
    "pred_default = np.where(pred_proba >= 0.5, 1, 0) # 0.5 is the default cut-off, equivalant to y_pred from above\n",
    "pred_th = np.where(pred_proba >= threshold_bayes, 1, 0) # Using the cut-off defined by the cost-minimal threshold function\n",
    "print(np.mean(pred_default), np.mean(pred_bayes)) # Shows the percentage of observations that are now predicted with the label 1, default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a47efe",
   "metadata": {},
   "source": [
    "### Sonstiges, keine Ahnung, wird wahrscheinlich nicht relevant sein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ea10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logit\n",
    "1/(1+math.e**-(clf_lr.fit(X_train, y_train).decision_function(X_test)))\n",
    "\n",
    "clf_lr.fit(X_train, y_train).decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eefa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class filter_binary_target:\n",
    "    def __init__(self, df, target):\n",
    "        self.target = target\n",
    "        self.data_head = df.head()\n",
    "\n",
    "    def auto_filter_binary_target(self):\n",
    "        print('Data must be in a clean pandas DataFrame. Categorical variables must be of data type bool or category. Continuous variables must be int64 or float64.')\n",
    "        data_no_target = df.drop(columns=self.target)\n",
    "        columns = ['Data Type', 'Metric', 'Score']\n",
    "        index = data_no_target.columns\n",
    "        result = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "        for col in data_no_target:\n",
    "            if data_no_target.dtypes[col] == 'bool' or data_no_target.dtypes[col].name == 'category':\n",
    "                result.loc[col, 'Data Type'] = \"discrete\"\n",
    "                result.loc[col, 'Metric'] = \"IV\"\n",
    "                result.loc[col, 'Score'] = self.IV_binary_target(feature=col)\n",
    "\n",
    "            if data_no_target.dtypes[col] == 'int64' or data_no_target.dtypes[col] == 'float64':\n",
    "                result.loc[col, 'Data Type'] = \"continuous\"\n",
    "                result.loc[col, 'Metric'] = \"Fisher\"\n",
    "                result.loc[col, 'Score'] = self.fisher_binary_target(feature=col)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def IV_binary_target(self, feature):  # same code as used above\n",
    "        data = pd.DataFrame()\n",
    "    \n",
    "        data['Count'] = df[feature].value_counts()\n",
    "        data['Bad'] = df.groupby([feature])[self.target].sum()\n",
    "        data['Good'] = data['Count'] - data['Bad']\n",
    "    \n",
    "        data[\"Distribution Bad\"] = data[\"Bad\"] / data[\"Bad\"].sum()\n",
    "        data[\"Distribution Good\"] = data[\"Good\"] / data[\"Good\"].sum()\n",
    "    \n",
    "        data['WOE'] = np.log(data[\"Distribution Good\"] / data[\"Distribution Bad\"])\n",
    "        data.replace({\"WOE\": {np.inf: 0, -np.inf: 0}})\n",
    "\n",
    "        data[\"IV\"] = data[\"WOE\"] * (data[\"Distribution Good\"] - data[\"Distribution Bad\"])\n",
    "\n",
    "        iv = data[\"IV\"].sum()\n",
    "\n",
    "        return iv\n",
    "\n",
    "    def fisher_binary_target(self, feature):\n",
    "        mu_0 = df.groupby(df[self.target])[feature].mean()[0]\n",
    "        mu_1 = df.groupby(df[self.target])[feature].mean()[1]\n",
    "        var_0 = df.groupby(df[self.target])[feature].var()[0]\n",
    "        var_1 = df.groupby(df[self.target])[feature].var()[1]\n",
    "\n",
    "        num = abs(mu_0 - mu_1)\n",
    "        den = (var_0 + var_1) ** 0.5\n",
    "        score = num/den\n",
    "    \n",
    "        return score\n",
    "\n",
    "    def pearson(self, feature):  # since our target is binary, we actually don't need this. However, if you would like to expand this class, you can use this code\n",
    "        mean_feature = df[feature].mean()\n",
    "        mean_target = df[self.target].mean()\n",
    "        num = ((df[feature] - mean_feature)*(df[self.target] - mean_target)).sum()\n",
    "        den = (((df[feature] - mean_feature)**2).sum() * ((df[self.target] - mean_target)**2).sum()) ** .5\n",
    "        rho = num/den\n",
    "        return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = filter_binary_target(df=train_df, target=\"BAD\")\n",
    "\n",
    "filter.auto_filter_binary_target()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
